/*
 * Copyright 1993-2015 NVIDIA Corporation.  All rights reserved.
 *
 * Please refer to the NVIDIA end user license agreement (EULA) associated
 * with this source code for terms and conditions that govern your use of
 * this software. Any use, reproduction, disclosure, or distribution of
 * this software and related documentation outside the terms of the EULA
 * is strictly prohibited.
 *
 */

#include <assert.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <cooperative_groups.h>

#include <helper_cuda.h>
#include "histogram_common.h"

////////////////////////////////////////////////////////////////////////////////
// Shortcut shared memory atomic addition functions
////////////////////////////////////////////////////////////////////////////////

#define TAG_MASK 0xFFFFFFFFU
/*
inline __device__ void addByte(int *s_WarpHist, int data, int threadTag)
{
    atomicAdd(s_WarpHist + data, 1);
}

inline __device__ void addWord(int *s_WarpHist, int data, int tag)
{
    addByte(s_WarpHist, (data >>  0) & 0xFFU, tag);
    addByte(s_WarpHist, (data >>  8) & 0xFFU, tag);
    addByte(s_WarpHist, (data >> 16) & 0xFFU, tag);
    addByte(s_WarpHist, (data >> 24) & 0xFFU, tag);
}
*/

__global__ void histogram256Kernel(int *d_PartialHistograms, int *d_Data, int dataCount)
{
    //// Handle to thread block group
    //cg::thread_block cta = cg::this_thread_block();
    //Per-warp subhistogram storage
    //__shared__ int s_Hist[6 * 256];
  __shared__ int s_Hist[256];
  //int offset = (threadIdx.x >> 5) * 256;

    //Clear shared memory storage for current threadblock before processing
#pragma unroll

    for (int i = 0; i < 256; i+= 32)
    {
        s_Hist[threadIdx.x + i] = 0;
    }

    //Cycle through the entire data set, update subhistograms for each warp
    //const int tag = threadIdx.x << 27;

    //cg::sync(cta);

    for (int pos = 0; pos < dataCount; pos += blockDim.x * gridDim.x)
    {
        int data = d_Data[pos + threadIdx.x];
        //s_Hist[byte(1,data)]++;
        //s_Hist[byte(2,data)]++;
        //s_Hist[byte(3,data)]++;
        //s_Hist[byte(4,data)]++;
        //s_Hist[1]++;
        //s_Hist[1]++;
        //s_Hist[1]++;
        //s_Hist[1]++;
        s_Hist[data % 256]++;
        s_Hist[(data >> 8) % 256]++;
        s_Hist[(data >> 16) % 256]++;
        s_Hist[(data >> 24) % 256]++;
        //s_Hist[offset + data % 256]++;
        //s_Hist[offset + (data >> 8) % 256]++;
        //s_Hist[offset + (data >> 16) % 256]++;
        //s_Hist[offset + (data >> 24) % 256]++;
    }

    //Merge per-warp histograms into per-block and write to global memory
    //cg::sync(cta);

for (int bin = 0; bin < 256; bin += 32)
    {
        int sum = 0;

        //for (int i = 0; i < 6; i++)
        //{
          sum += s_Hist[bin + threadIdx.x];// & TAG_MASK;
          //}

        d_PartialHistograms[bin + threadIdx.x] = sum;
    }
}

/*
////////////////////////////////////////////////////////////////////////////////
// Merge histogram256() output
// Run one threadblock per bin; each threadblock adds up the same bin counter
// from every partial histogram. Reads are uncoalesced, but mergeHistogram256
// takes only a fraction of total processing time
////////////////////////////////////////////////////////////////////////////////
#define MERGE_THREADBLOCK_SIZE 256

__global__ void mergeHistogram256Kernel(
    int *d_Histogram,
    int *d_PartialHistograms,
    int histogramCount
)
{
    // Handle to thread block group
    //cg::thread_block cta = cg::this_thread_block();

    int sum = 0;

    for (int i = threadIdx.x; i < histogramCount; i += MERGE_THREADBLOCK_SIZE)
    {
        sum += d_PartialHistograms[blockIdx.x + i * HISTOGRAM256_BIN_COUNT];
    }

    __shared__ int data[MERGE_THREADBLOCK_SIZE];
    data[threadIdx.x] = sum;

    for (int stride = MERGE_THREADBLOCK_SIZE / 2; stride > 0; stride >>= 1)
    {
        cg::sync(cta);

        if (threadIdx.x < stride)
        {
            data[threadIdx.x] += data[threadIdx.x + stride];
        }
    }

    if (threadIdx.x == 0)
    {
        d_Histogram[blockIdx.x] = data[0];
    }
}
*/
/*
////////////////////////////////////////////////////////////////////////////////
// Host interface to GPU histogram
////////////////////////////////////////////////////////////////////////////////
//histogram256kernel() intermediate results buffer
static const int PARTIAL_HISTOGRAM256_COUNT = 240;
static int *d_PartialHistograms;

//Internal memory allocation
extern "C" void initHistogram256(void)
{
    checkCudaErrors(cudaMalloc((void **)&d_PartialHistograms, PARTIAL_HISTOGRAM256_COUNT * HISTOGRAM256_BIN_COUNT * sizeof(int)));
}

//Internal memory deallocation
extern "C" void closeHistogram256(void)
{
    checkCudaErrors(cudaFree(d_PartialHistograms));
}

extern "C" void histogram256(
    int *d_Histogram,
    void *d_Data,
    int byteCount
)
{
    assert(byteCount % sizeof(int) == 0);
    histogram256Kernel<<<PARTIAL_HISTOGRAM256_COUNT, HISTOGRAM256_THREADBLOCK_SIZE>>>(
        d_PartialHistograms,
        (int *)d_Data,
        byteCount / sizeof(int)
    );
    getLastCudaError("histogram256Kernel() execution failed\n");

    mergeHistogram256Kernel<<<HISTOGRAM256_BIN_COUNT, MERGE_THREADBLOCK_SIZE>>>(
        d_Histogram,
        d_PartialHistograms,
        PARTIAL_HISTOGRAM256_COUNT
    );
    getLastCudaError("mergeHistogram256Kernel() execution failed\n");
}
*/
